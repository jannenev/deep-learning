{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis on Twitter data - predict positive/negative sentiment of tweet\n",
    "# using Twitter data from: \n",
    "# \"The Twitter Sentiment Analysis Dataset contains 1,578,627 classified tweets, \n",
    "#  each row is marked as 1 for positive sentiment and 0 for negative sentiment\"\n",
    "# linked in\n",
    "# http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/\n",
    "# http://thinknook.com/wp-content/uploads/2012/09/Sentiment-Analysis-Dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "numpy.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 8836: expected 4 fields, saw 5\\n'\n",
      "b'Skipping line 535882: expected 4 fields, saw 7\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded lines:\n",
      "1578612\n"
     ]
    }
   ],
   "source": [
    "# read in data\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "zf = zipfile.ZipFile('C:/twitter/Sentiment-Analysis-Dataset.zip') \n",
    "df = pd.read_csv(zf.open('Sentiment Analysis Dataset.csv'), quotechar='\"', sep=',', error_bad_lines=False)\n",
    "print('Loaded lines:')\n",
    "print(len(df))\n",
    "\n",
    "# 2 lines fail and are skipped\n",
    "# csv line 8835,1,Kaggle,\"\"\" Brokeback Mountain \"\" is a great short story and explains more, oddly enough, than the movie does, even though both cover the same chronological ground.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of inputs:\n",
      "100000\n",
      "Number of words used:\n",
      "1000\n",
      "X_train length:\n",
      "90000\n",
      "X_test length:\n",
      "10000\n",
      "Length of features in each item\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "# how many tweets to use, large amount may run out of memory, 200.000 use around 8 GB \n",
    "# 100.000 sample 1000 top_words use around 8 GB\n",
    "# 200.000 sample 3000 top_words around 16 GB\n",
    "sample_size = 100000\n",
    "# how many most common words to use\n",
    "top_words = 1000\n",
    "\n",
    "sample = df.sample(sample_size)\n",
    "mydataX = sample['SentimentText']\n",
    "y = sample['Sentiment']\n",
    "\n",
    "# transform pandas.core.series.Series to list\n",
    "y = pd.Series.tolist(y)\n",
    "mydataX = pd.Series.tolist(mydataX)\n",
    "\n",
    "# translate text words into integer vectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "# create the tokenizer\n",
    "t = Tokenizer(num_words=top_words,\n",
    "                filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                lower=True,\n",
    "                split=\" \",\n",
    "                char_level=False)\n",
    "\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(mydataX)\n",
    "# encode the texts\n",
    "encoded = t.texts_to_matrix(mydataX)\n",
    "\n",
    "# lenght: number of docs * number of words used\n",
    "print('Number of inputs:')\n",
    "print(len(encoded))\n",
    "print('Number of words used:')\n",
    "print(len(encoded[1])) # same as print(t.num_words)\n",
    "\n",
    "# split datasample into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(encoded, y, test_size=0.1, random_state=1)\n",
    "\n",
    "print('X_train length:')\n",
    "print(len(X_train))\n",
    "print('X_test length:')\n",
    "print(len(X_test))\n",
    "\n",
    "# truncate and pad input sequences\n",
    "max_review_length = top_words\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "\n",
    "# confirm length of each item is max_review_length\n",
    "print('Length of features in each item')\n",
    "print(len(X_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 100)               100100    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 100,201\n",
      "Trainable params: 100,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Dense(input_dim=top_words, units=100, activation='tanh'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "90000/90000 [==============================] - 4s 41us/step - loss: 0.5270 - acc: 0.7441\n",
      "Epoch 2/10\n",
      "90000/90000 [==============================] - 3s 37us/step - loss: 0.5068 - acc: 0.7576\n",
      "Epoch 3/10\n",
      "90000/90000 [==============================] - 3s 37us/step - loss: 0.5033 - acc: 0.7606\n",
      "Epoch 4/10\n",
      "90000/90000 [==============================] - 3s 37us/step - loss: 0.5001 - acc: 0.7622\n",
      "Epoch 5/10\n",
      "90000/90000 [==============================] - 3s 38us/step - loss: 0.4945 - acc: 0.7629\n",
      "Epoch 6/10\n",
      "90000/90000 [==============================] - 3s 38us/step - loss: 0.4882 - acc: 0.7655\n",
      "Epoch 7/10\n",
      "90000/90000 [==============================] - 3s 38us/step - loss: 0.4828 - acc: 0.7680\n",
      "Epoch 8/10\n",
      "90000/90000 [==============================] - 3s 38us/step - loss: 0.4786 - acc: 0.7698\n",
      "Epoch 9/10\n",
      "90000/90000 [==============================] - 3s 39us/step - loss: 0.4751 - acc: 0.7720\n",
      "Epoch 10/10\n",
      "90000/90000 [==============================] - 3s 37us/step - loss: 0.4716 - acc: 0.7729\n",
      "Accuracy: 76.95%\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=10, batch_size=100)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=5, batch_size=100)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "# 100.000 sample 1000 top words : 76,7% accuracy on 10 epochs\n",
    "# 150.000 sample 2000 top words : 77,8 % on 10 epochs, 20 epochs overfits\n",
    "# 200.00         3000           : 79.1 % on 10 epochs, 15 epochs overfits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py36)",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
